# Tabla de Complejidades de Algoritmos âš¡

En AnÃ¡lisis y DiseÃ±o de Algoritmos (ADA) es clave entender cÃ³mo **crece el tiempo de ejecuciÃ³n** a medida que aumenta la entrada (`N`).  
Estas son las complejidades mÃ¡s comunes, ordenadas de **mÃ¡s rÃ¡pida a mÃ¡s lenta**.

---

## ğŸ“Š Complejidades Comunes

| NotaciÃ³n | Nombre | DescripciÃ³n | Ejemplo tÃ­pico |
|----------|--------|-------------|----------------|
| **O(1)** | Constante | Siempre tarda lo mismo, sin importar `N`. | Acceder a un elemento de un array por Ã­ndice |
| **O(log N)** | LogarÃ­tmica | Crece muy despacio aunque `N` sea grande. | BÃºsqueda binaria |
| **O(N)** | Lineal | Tiempo proporcional a la cantidad de datos. | Recorrer una lista completa |
| **O(N log N)** | Casi Lineal | Un poco mÃ¡s que lineal, pero mucho mÃ¡s eficiente que cuadrÃ¡tica. | Quicksort, Mergesort |
| **O(NÂ²)** | CuadrÃ¡tica | Crece rÃ¡pido, cada elemento se compara con todos. | Doble bucle anidado |
| **O(NÂ³)** | CÃºbica | AÃºn mÃ¡s costosa que cuadrÃ¡tica. | Multiplicar matrices naive |
| **O(2^N)** | Exponencial | Imposible de manejar con `N` grandes. | Algoritmos de fuerza bruta (subset sum, backtracking) |
| **O(N!)** | Factorial | La mÃ¡s costosa, crece brutalmente rÃ¡pido. | Generar todas las permutaciones |

---

## âš–ï¸ Resumen visual

- ğŸš€ **O(1), O(log N)** â†’ ideales, muy eficientes.  
- ğŸ™‚ **O(N), O(N log N)** â†’ aceptables en la prÃ¡ctica.  
- âš ï¸ **O(NÂ²), O(NÂ³)** â†’ solo sirven para `N` pequeÃ±os.  
- âŒ **O(2^N), O(N!)** â†’ impracticables para `N` moderados.  

---

## ğŸ¯ Tip
En concursos o problemas tipo ADA, **siempre intenta reducir** la complejidad hacia **O(N log N)** o menor.  
Un algoritmo O(NÂ²) puede funcionar con `N = 1000`, pero fracasar con `N = 10^5`.  
